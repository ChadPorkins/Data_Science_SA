---
title: "Data science project - Suicidial attacks"
author: "Mr. Jakob Alderson & MR. Tal Bracha"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

```{r, echo= FALSE, message=FALSE}
library(tidyverse)
library(mapview)
library(sf)
library(ggplot2)
library(ggmap)
library(tseries)
library(car)
library(moments)
library(writexl)
library(AID)
```


## inro

why its impotent

## litetature 

##hypothesis

## Suicidal attacks global or middle eastern problem?

```{r}
points = read.csv("attacks.csv")
points = points[!is.na(points$latitude), ]
points = points[!is.na(points$longitude), ]
```

```{r, warning=FALSE}

world <- map_data("world")
   ggplot() +
     geom_map(
       data = world, map = world,
       aes(long, lat, map_id = region),
       color = "black", fill = "lightgray", size = 0.1
     ) +
     geom_point(
       data = points,
       aes(longitude, latitude), color = "blue",
       alpha = 0.7
     ) 
```

Point density:
```{r}
ggplot(points, aes(x = longitude, y = latitude)) + 
     geom_point() + 
     coord_equal() + 
     xlab('longitude') + 
     ylab('latitude') + 
     stat_density2d(aes(fill = ..level..), alpha = 100,
                    geom = "polygon", data = points) + 
     scale_fill_viridis_c() + 
     theme(legend.position = 'none') 
```

The middle east  is the brightest

## Data Cleaning and Normaliry

Before cleanig:

```{r}
attacks = read.csv("attacks.csv")
head(as_tibble(attacks))
```
 
After some data organization:
```{r}
attacks = as_tibble(attacks) %>%
  select(event_id,wounded_low,wounded_high,killed_low,killed_high) %>%
  filter(wounded_low >= 0) %>%
  filter(wounded_high >= 0) %>%
  filter(killed_low >= 0) %>%
  filter(killed_high >= 0) %>%
  mutate(wounded_average = round(((wounded_low + wounded_high)/2) )) %>%
  mutate(killed_average = round(((killed_low + killed_high)/2) )) %>% 
  mutate(casualties = wounded_average + killed_average) %>%
  filter(casualties >= 1) %>%
  select(event_id,wounded_average,killed_average,casualties)
 head(attacks)
```

## normality visualtization 

```{r}
par(mfrow = c(1,3))
hist(attacks$wounded_average, breaks = length(attacks$wounded_average))
hist(attacks$killed_average, breaks = length(attacks$killed_average))
hist(attacks$casualties, breaks = length(attacks$casualties))

```

The data is Exponential - Gamma distributions, we have a massive possite right skew 

```{r}
par(mfrow = c(1,3))
qqPlot(attacks$wounded_average, id= FALSE)
qqPlot(attacks$killed_average, id= FALSE)
qqPlot(attacks$casualties, id= FALSE)
```



## sqrt data transform 

```{r}
par(mfrow = c(1,3))
hist(sqrt(attacks$wounded_average + 1 ))
hist(sqrt(attacks$killed_average + 1 ))
hist(sqrt(attacks$casualties))

```

## log data transform  

```{r}
par(mfrow = c(1,3))
hist(log10(attacks$wounded_average + 1 ))
hist(log10(attacks$killed_average + 1 ))
hist(log10(attacks$casualties))

```

## inverse data transform 
```{r}
par(mfrow = c(1,3))
hist(1/(attacks$wounded_average + 1 ))
hist(1/(attacks$killed_average + 1 ))
hist(1/(attacks$casualties))

```

## log 10 looks like nomal disterbution, but whats the problem?

too many repetitions

```{r}
qqPlot(log10(attacks$casualties), id= FALSE)
```

## We need to Split our data
```{r}
attacks2 = attacks %>%
  count(casualties, sort = TRUE) #%>%
  #filter(casualties < 800)
head(attacks2)

```

## creating new tibblies
```{r}
x = filter(attacks2, n>=400 ) %>%
  select(casualties) %>%
   pull()
```

```{r}

very_common_400  =  filter(attacks, casualties %in% x )

test1 = very_common_400 %>%
  select(wounded_average ,killed_average , casualties) %>%
  apply(2, agostino.test)

test1

```

```{r}
y = filter(attacks2, n<=400 & n>=300 ) %>%
  select(casualties) %>%
   pull()
```


```{r}
very_common_300 = filter(attacks, casualties %in% y )

test2 = very_common_300 %>%
  select(wounded_average ,killed_average , casualties) %>%
  apply(2, agostino.test)

test2

```


```{r}
z = filter(attacks2, n<=300 & n>=100 ) %>%
  select(casualties) %>%
   pull()
```


```{r}
very_common_200 = filter(attacks, casualties %in% z )

test3 = very_common_200 %>%
  select(wounded_average ,killed_average , casualties) %>%
  apply(2, agostino.test)

test3

```


```{r}
q = filter(attacks2, n<=100 & n>=80 ) %>%
  select(casualties) %>%
   pull()
```


```{r}
very_common_100 = filter(attacks, casualties %in% q )

test4 = very_common_100 %>%
  select(wounded_average ,killed_average , casualties) %>%
  apply(2, agostino.test)

test4

```

```{r}
p = filter(attacks2, n<=80 & n>50 ) %>%
  select(casualties) %>%
   pull()
```


```{r}
very_common_0 = filter(attacks, casualties %in% p )

test5 = very_common_0 %>%
  select(wounded_average ,killed_average , casualties) %>%
  apply(2, agostino.test)

test5

```


```{r}
k = filter(attacks2, n<=50 & n>30 ) %>%
  select(casualties) %>%
   pull()
```


```{r}
very_common_20 = filter(attacks, casualties %in% k )

test6 = very_common_20 %>%
  select(wounded_average ,killed_average , casualties) %>%
  apply(2, agostino.test)

test6

```


```{r}
e = filter(attacks2, n<=30 & n>21 ) %>%
  select(casualties) %>%
   pull()
```


```{r}
very_common_e = filter(attacks, casualties %in% e )

test7 = very_common_e %>%
  select(wounded_average ,killed_average , casualties) %>%
  apply(2, agostino.test)

test7

```


```{r}
r = filter(attacks2, n<=21 & n>16 ) %>%
  select(casualties) %>%
   pull()
```


```{r}
very_common_r = filter(attacks, casualties %in% r )

test8 = very_common_r %>%
  select(wounded_average ,killed_average , casualties) %>%
  apply(2, agostino.test)

test8

```


```{r}
t = filter(attacks2, n<=16 & n>0 ) %>%
  select(casualties) %>%
   pull()
```


```{r}
very_common_t = filter(attacks, casualties %in% t )

agostino.test(1/(very_common_t$casualties))


```



